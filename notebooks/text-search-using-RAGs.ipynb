{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cohere\n",
    "import hnswlib\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.text import partition_text\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# co = cohere.Client(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Datastore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents.\n",
    "\n",
    "    Parameters:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents.\n",
    "    docs (list): A list of dictionaries representing the documents, with 'title', 'content', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the documents.\n",
    "    docs_len (int): The number of documents in the collection.\n",
    "    index (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the documents using the Cohere API.\n",
    "    index(): Indexes the documents for efficient retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents  # raw documents\n",
    "        self.chunks = []            # chunked version of documents\n",
    "        self.chunks_embs = []       # embeddings of chunked documents\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()  # load raw documents and break into chunks\n",
    "        self.embed() # generate embeddings for each chunk\n",
    "        self.index() # store embeddings in an index\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for source in self.raw_documents:\n",
    "            elements = partition_text(filename=source[\"filename\"])\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.chunks.append(\n",
    "                    {\n",
    "                        \"title\": source[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": source[\"filename\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.chunks_len = len(self.chunks)\n",
    "\n",
    "        for i in range(0, self.chunks_len, batch_size):\n",
    "            batch = self.chunks[i : min(i + batch_size, self.chunks_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            chunks_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.chunks_embs.extend(chunks_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing documents...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.chunks_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.chunks_embs, list(range(len(self.chunks_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
    "\n",
    "        return self.idx\n",
    "\n",
    "    def search_and_rerank(self, query: str) -> List[Dict[str, str]]:\n",
    "        # SEARCH\n",
    "        query_emb = co.embed(\n",
    "                  texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
    "              ).embeddings\n",
    "\n",
    "        chunk_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # RERANK\n",
    "        chunks_to_rerank = [self.chunks[chunk_id][\"text\"] for chunk_id in chunk_ids]\n",
    "\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=chunks_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v2.0\",\n",
    "        )\n",
    "\n",
    "        chunk_ids_reranked = [chunk_ids[result.index] for result in rerank_results]\n",
    "\n",
    "        chunks_retrieved = []\n",
    "        for chunk_id in chunk_ids_reranked:\n",
    "            chunks_retrieved.append(\n",
    "                {\n",
    "                \"title\": self.chunks[chunk_id][\"title\"],\n",
    "                \"text\": self.chunks[chunk_id][\"text\"],\n",
    "                \"filename\": self.chunks[chunk_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing documents...\n",
      "Indexing complete with 16 documents.\n"
     ]
    }
   ],
   "source": [
    "sources = [\n",
    "    {\n",
    "        \"title\": \"Data Quarkle\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md\"},\n",
    "    {\n",
    "        \"title\": \"RAGs\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/undestanding-rags.md\"},\n",
    "        {\n",
    "        \"title\": \"sample\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/sample.md\"},    \n",
    "]\n",
    "\n",
    "# Create an instance of the Datastore class with the given sources\n",
    "datastore = Datastore(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x7f12f5a70a60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f12ee926890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f12ee924100>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f12ee9261a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f12ee926110>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f12ee925fc0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_text(filename='/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Data Quarkle',\n",
       "  'text': 'Introducing \"DataQuarkle\" – a fantastical approach to data engineering that transcends conventional boundaries and unleashes the power of cosmic data manipulation.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'In the realm of DataQuarkle, innovation knows no bounds. From \"Datafusion\" spells that merge disparate data sources into a harmonious symphony of insights to \"Datamancy\" rituals that foresee future trends through the cosmic currents, DataQuarkle pushes the boundaries of what is possible in the realm of data engineering.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'DataQuarkle harnesses the mysterious energy of \"Gigavortex,\" a celestial phenomenon that swirls through the cosmos, weaving together streams of data from across the universe. Powered by \"Quarkblasters,\" quantum-enhanced data processors, DataQuarkle pioneers the frontier of \"Datastral Travel,\" enabling data engineers to traverse the vast expanse of the Dataverse in the blink of an eye.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore.search_and_rerank(\"quarkle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, datastore: Datastore):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the Chatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        storage (Storage): An instance of the Storage class.\n",
    "\n",
    "        \"\"\"\n",
    "        self.datastore = datastore\n",
    "        self.conversation_id = str(uuid.uuid4())\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the chatbot application.\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get the user message\n",
    "            message = input(\"User: \")\n",
    "\n",
    "            # Typing \"quit\" ends the conversation\n",
    "            if message.lower() == \"quit\":\n",
    "                print(\"Ending chat.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"User: {message}\")\n",
    "\n",
    "            # Generate search queries, if any\n",
    "            response_queries = co.chat(message=message, search_queries_only=True)\n",
    "\n",
    "            if response_queries.search_queries:\n",
    "                print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "                # Get the query(s)\n",
    "                queries = []\n",
    "                for search_query in response_queries.search_queries:\n",
    "                    queries.append(search_query[\"text\"])\n",
    "\n",
    "                # Retrieve documents for each query\n",
    "                chunks = []\n",
    "                for query in queries:\n",
    "                    chunks.extend(self.datastore.search_and_rerank(query))\n",
    "            \n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    documents=chunks,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            # Print the chatbot response\n",
    "            print(\"\\nChatbot:\")\n",
    "            \n",
    "            citations_flag = False\n",
    "            \n",
    "            for event in response:\n",
    "                                \n",
    "                # Text\n",
    "                if event.event_type == \"text-generation\":\n",
    "                    print(event.text, end=\"\")\n",
    "\n",
    "                # Citations\n",
    "                if event.event_type == \"citation-generation\":\n",
    "                    if not citations_flag:\n",
    "                        print(\"\\n\\nCITATIONS:\")\n",
    "                        citations_flag = True\n",
    "                    print(event.citations[0])\n",
    "            \n",
    "            # Documents\n",
    "            if citations_flag:\n",
    "                print(\"\\n\\nDOCUMENTS:\")\n",
    "                documents = [{'id': doc['id'],\n",
    "                                'text': doc['text'][:50] + '...',\n",
    "                                'title': doc['title'],\n",
    "                                'url': doc['filename']} \n",
    "                                for doc in response.documents]\n",
    "                for doc in documents:\n",
    "                    print(doc)\n",
    "\n",
    "            print(f\"\\n{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: data quarkle\n",
      "Retrieving information...\n",
      "Chatbot:\n",
      "DataQuarkle is an innovative approach to data engineering that leverages the power of cosmic data manipulation. It harnesses the energy of a celestial phenomenon called \"Gigavortex\" and channels it through \"Quarkblasters\"—quantum-enhanced data processors. This ground-breaking method explores the realm of \"Datastral Travel\", enabling data engineers to traverse the Dataverse with unprecedented speed and efficiency. \n",
      "\n",
      "DataQuarkle also introduces exciting techniques like \"Datafusion\" spells, which merge diverse data sources into a harmonious symphony of insights, and \"Datamancy\" rituals that utilise cosmic currents to predict future trends. With DataQuarkle, the possibilities for data engineering are limitless, opening up a new dimension in the understanding and manipulation of data.\n",
      "\n",
      "CITATIONS:\n",
      "{'start': 18, 'end': 37, 'text': 'innovative approach', 'document_ids': ['doc_0', 'doc_2']}\n",
      "{'start': 41, 'end': 57, 'text': 'data engineering', 'document_ids': ['doc_0', 'doc_1', 'doc_2']}\n",
      "{'start': 86, 'end': 111, 'text': 'cosmic data manipulation.', 'document_ids': ['doc_0']}\n",
      "{'start': 115, 'end': 135, 'text': 'harnesses the energy', 'document_ids': ['doc_1']}\n",
      "{'start': 141, 'end': 181, 'text': 'celestial phenomenon called \"Gigavortex\"', 'document_ids': ['doc_1']}\n",
      "{'start': 206, 'end': 221, 'text': '\"Quarkblasters\"', 'document_ids': ['doc_1']}\n",
      "{'start': 239, 'end': 255, 'text': 'data processors.', 'document_ids': ['doc_1']}\n",
      "{'start': 306, 'end': 324, 'text': '\"Datastral Travel\"', 'document_ids': ['doc_1']}\n",
      "{'start': 335, 'end': 349, 'text': 'data engineers', 'document_ids': ['doc_1']}\n",
      "{'start': 353, 'end': 375, 'text': 'traverse the Dataverse', 'document_ids': ['doc_1']}\n",
      "{'start': 472, 'end': 491, 'text': '\"Datafusion\" spells', 'document_ids': ['doc_2']}\n",
      "{'start': 499, 'end': 525, 'text': 'merge diverse data sources', 'document_ids': ['doc_2']}\n",
      "{'start': 533, 'end': 564, 'text': 'harmonious symphony of insights', 'document_ids': ['doc_2']}\n",
      "{'start': 570, 'end': 589, 'text': '\"Datamancy\" rituals', 'document_ids': ['doc_2']}\n",
      "{'start': 603, 'end': 618, 'text': 'cosmic currents', 'document_ids': ['doc_2']}\n",
      "{'start': 622, 'end': 644, 'text': 'predict future trends.', 'document_ids': ['doc_2']}\n",
      "\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Introducing \"DataQuarkle\" – a fantastical approach...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "{'id': 'doc_2', 'text': 'In the realm of DataQuarkle, innovation knows no b...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "{'id': 'doc_1', 'text': 'DataQuarkle harnesses the mysterious energy of \"Gi...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "User: how is data fusion related to data void\n",
      "Retrieving information...\n",
      "Chatbot:\n",
      "Data fusion and data void are interconnected concepts in the imaginative realm of Data Quarkle. Data fusion spells are employed to merge different data sources, revealing insightful harmonies that can help to guide decisions. Conversely, the data void, referred to as the \"Datavoid,\" is a mysterious and intimidating abyss where data is lost and vanishes into nonexistence. \n",
      "\n",
      "While data fusion creates new meaningful connections, the data void represents the absence of them, serving as a cautionary tale for data engineers and a test of their courage. Armed with the legendary \"DataScepter,\" they dare to venture into the data void's depths, hoping to unlock the secrets of lost data and bring them back into the insightful realm of data fusion.\n",
      "\n",
      "CITATIONS:\n",
      "{'start': 82, 'end': 95, 'text': 'Data Quarkle.', 'document_ids': ['doc_0', 'doc_1']}\n",
      "{'start': 108, 'end': 114, 'text': 'spells', 'document_ids': ['doc_1']}\n",
      "{'start': 131, 'end': 159, 'text': 'merge different data sources', 'document_ids': ['doc_1']}\n",
      "{'start': 171, 'end': 191, 'text': 'insightful harmonies', 'document_ids': ['doc_1']}\n",
      "{'start': 272, 'end': 281, 'text': '\"Datavoid', 'document_ids': ['doc_0']}\n",
      "{'start': 289, 'end': 299, 'text': 'mysterious', 'document_ids': ['doc_0']}\n",
      "{'start': 304, 'end': 316, 'text': 'intimidating', 'document_ids': ['doc_0']}\n",
      "{'start': 317, 'end': 322, 'text': 'abyss', 'document_ids': ['doc_0']}\n",
      "{'start': 329, 'end': 341, 'text': 'data is lost', 'document_ids': ['doc_0']}\n",
      "{'start': 346, 'end': 373, 'text': 'vanishes into nonexistence.', 'document_ids': ['doc_0']}\n",
      "{'start': 509, 'end': 523, 'text': 'data engineers', 'document_ids': ['doc_0']}\n",
      "{'start': 578, 'end': 590, 'text': '\"DataScepter', 'document_ids': ['doc_0']}\n",
      "{'start': 606, 'end': 641, 'text': \"venture into the data void's depths\", 'document_ids': ['doc_0']}\n",
      "{'start': 653, 'end': 684, 'text': 'unlock the secrets of lost data', 'document_ids': ['doc_0']}\n",
      "\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'But beware the \"Datavoid,\" a dark abyss lurking at...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "{'id': 'doc_1', 'text': 'In the realm of DataQuarkle, innovation knows no b...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "User: is data quarkle real?\n",
      "Retrieving information...\n",
      "Chatbot:\n",
      "Data Quarkle is an imaginative and creative approach to data engineering. It introduces captivating concepts such as harnessing cosmic energy and quantum-enhanced data processors, paving the way for an extraordinary journey through the Dataverse. While Data Quarkle may not be a physically real phenomenon, it certainly is an intriguing and fun way to conceptualise data manipulation and engineering.\n",
      "\n",
      "CITATIONS:\n",
      "{'start': 19, 'end': 43, 'text': 'imaginative and creative', 'document_ids': ['doc_0', 'doc_1', 'doc_2']}\n",
      "{'start': 56, 'end': 73, 'text': 'data engineering.', 'document_ids': ['doc_0', 'doc_1', 'doc_2']}\n",
      "{'start': 117, 'end': 141, 'text': 'harnessing cosmic energy', 'document_ids': ['doc_1']}\n",
      "{'start': 146, 'end': 178, 'text': 'quantum-enhanced data processors', 'document_ids': ['doc_1']}\n",
      "{'start': 216, 'end': 246, 'text': 'journey through the Dataverse.', 'document_ids': ['doc_1']}\n",
      "\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Introducing \"DataQuarkle\" – a fantastical approach...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "{'id': 'doc_1', 'text': 'DataQuarkle harnesses the mysterious energy of \"Gi...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "{'id': 'doc_2', 'text': 'In the realm of DataQuarkle, innovation knows no b...', 'title': 'Data Quarkle', 'url': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m Chatbot(datastore)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 20\u001b[0m, in \u001b[0;36mChatbot.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mRuns the chatbot application.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Get the user message\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Typing \"quit\" ends the conversation\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Chatbot class with the Datastore instance\n",
    "chatbot = Chatbot(datastore)\n",
    "\n",
    "# Run the chatbot\n",
    "chatbot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
