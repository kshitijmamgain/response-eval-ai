{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "import hnswlib\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.text import partition_text\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# co = cohere.Client(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Datastore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents.\n",
    "\n",
    "    Parameters:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents.\n",
    "    docs (list): A list of dictionaries representing the documents, with 'title', 'content', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the documents.\n",
    "    docs_len (int): The number of documents in the collection.\n",
    "    index (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the documents using the Cohere API.\n",
    "    index(): Indexes the documents for efficient retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents  # raw documents\n",
    "        self.chunks = []            # chunked version of documents\n",
    "        self.chunks_embs = []       # embeddings of chunked documents\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()  # load raw documents and break into chunks\n",
    "        self.embed() # generate embeddings for each chunk\n",
    "        self.index() # store embeddings in an index\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for source in self.raw_documents:\n",
    "            elements = partition_text(filename=source[\"filename\"])\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.chunks.append(\n",
    "                    {\n",
    "                        \"title\": source[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": source[\"filename\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.chunks_len = len(self.chunks)\n",
    "\n",
    "        for i in range(0, self.chunks_len, batch_size):\n",
    "            batch = self.chunks[i : min(i + batch_size, self.chunks_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            chunks_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.chunks_embs.extend(chunks_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing documents...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.chunks_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.chunks_embs, list(range(len(self.chunks_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
    "\n",
    "        return self.idx\n",
    "\n",
    "    def search_and_rerank(self, query: str) -> List[Dict[str, str]]:\n",
    "        # SEARCH\n",
    "        query_emb = co.embed(\n",
    "                  texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
    "              ).embeddings\n",
    "\n",
    "        chunk_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # RERANK\n",
    "        chunks_to_rerank = [self.chunks[chunk_id][\"text\"] for chunk_id in chunk_ids]\n",
    "\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=chunks_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v2.0\",\n",
    "        )\n",
    "\n",
    "        chunk_ids_reranked = [chunk_ids[result.index] for result in rerank_results]\n",
    "\n",
    "        chunks_retrieved = []\n",
    "        for chunk_id in chunk_ids_reranked:\n",
    "            chunks_retrieved.append(\n",
    "                {\n",
    "                \"title\": self.chunks[chunk_id][\"title\"],\n",
    "                \"text\": self.chunks[chunk_id][\"text\"],\n",
    "                \"filename\": self.chunks[chunk_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing documents...\n",
      "Indexing complete with 41 documents.\n"
     ]
    }
   ],
   "source": [
    "sources = [\n",
    "    {\n",
    "        \"title\": \"Data Quarkle\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md\"},\n",
    "    {\n",
    "        \"title\": \"RAGs\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/undestanding-rags.md\"},\n",
    "        {\n",
    "        \"title\": \"sample\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/sample.md\"},    \n",
    "        {\n",
    "        \"title\": \"de-concepts\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/understanding-de-concepts.md\"}, \n",
    "]\n",
    "\n",
    "# Create an instance of the Datastore class with the given sources\n",
    "datastore = Datastore(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x7f9f038002b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f9f03802530>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f9f03802a70>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f9f03800fd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f9f03803a90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f9f03803790>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_text(filename='/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Data Quarkle',\n",
       "  'text': '# Data Quarkle - a new concept Introducing \"DataQuarkle\" – a fantastical approach to data engineering that transcends conventional boundaries and unleashes the power of cosmic data manipulation.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'In the realm of DataQuarkle, innovation knows no bounds. From \"Datafusion\" spells that merge disparate data sources into a harmonious symphony of insights to \"Datamancy\" rituals that foresee future trends through the cosmic currents, DataQuarkle pushes the boundaries of what is possible in the realm of data engineering.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'DataQuarkle harnesses the mysterious energy of \"Gigavortex,\" a celestial phenomenon that swirls through the cosmos, weaving together streams of data from across the universe. Powered by \"Quarkblasters,\" quantum-enhanced data processors, DataQuarkle pioneers the frontier of \"Datastral Travel,\" enabling data engineers to traverse the vast expanse of the Dataverse in the blink of an eye.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore.search_and_rerank(\"quarkle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, datastore: Datastore):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the Chatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        storage (Storage): An instance of the Storage class.\n",
    "\n",
    "        \"\"\"\n",
    "        self.datastore = datastore\n",
    "        self.conversation_id = str(uuid.uuid4())\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the chatbot application.\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get the user message\n",
    "            message = input()\n",
    "\n",
    "            # Typing \"quit\" ends the conversation\n",
    "            if message.lower() == \"quit\":\n",
    "                print(\"Ending chat.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"User: {message}\")\n",
    "\n",
    "            # Generate search queries, if any\n",
    "            response_queries = co.chat(message=message, search_queries_only=True)\n",
    "\n",
    "            if response_queries.search_queries:\n",
    "                print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "                # Get the query(s)\n",
    "                queries = []\n",
    "                for search_query in response_queries.search_queries:\n",
    "                    queries.append(search_query[\"text\"])\n",
    "\n",
    "                # Retrieve documents for each query\n",
    "                chunks = []\n",
    "                for query in queries:\n",
    "                    chunks.extend(self.datastore.search_and_rerank(query))\n",
    "            \n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    documents=chunks,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            # Print the chatbot response\n",
    "            print(\"\\nChatbot:\")\n",
    "            \n",
    "            citations_flag = False\n",
    "            \n",
    "            for event in response:\n",
    "                                \n",
    "                # Text\n",
    "                if event.event_type == \"text-generation\":\n",
    "                    print(event.text, end=\"\")\n",
    "\n",
    "                # Citations\n",
    "                if event.event_type == \"citation-generation\":\n",
    "                    if not citations_flag:\n",
    "                        print(\"\\n\\nCITATIONS:\")\n",
    "                        citations_flag = True\n",
    "                    print(event.citations[0])\n",
    "            \n",
    "            # Documents\n",
    "            if citations_flag:\n",
    "                print(\"\\n\\nDOCUMENTS:\")\n",
    "                documents = [{'id': doc['id'],\n",
    "                                'text': doc['text'][:50] + '...',\n",
    "                                'title': doc['title'],\n",
    "                                'url': doc['filename']} \n",
    "                                for doc in response.documents]\n",
    "                for doc in documents:\n",
    "                    print(doc)\n",
    "\n",
    "            print(f\"\\n{'-'*100}\\n\")\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of the Chatbot class with the Datastore instance\n",
    "chatbot = Chatbot(datastore)\n",
    "\n",
    "# # Run the chatbot\n",
    "# chatbot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is data warehousing and how is it different from traditional database?\n",
      "Retrieving information...\n",
      "Chatbot:\n",
      "Data warehousing involves the process of collecting, storing and managing data from various sources in order to aid decision-making processes. \n",
      "\n",
      "Data warehouses are different from traditional databases because they are optimised for analytical queries rather than transactional operations. They enable businesses to analyse historical data trends.\n",
      "\n",
      "CITATIONS:\n",
      "{'start': 41, 'end': 51, 'text': 'collecting', 'document_ids': ['doc_0']}\n",
      "{'start': 53, 'end': 60, 'text': 'storing', 'document_ids': ['doc_0']}\n",
      "{'start': 65, 'end': 99, 'text': 'managing data from various sources', 'document_ids': ['doc_0']}\n",
      "{'start': 112, 'end': 142, 'text': 'aid decision-making processes.', 'document_ids': ['doc_0']}\n",
      "{'start': 219, 'end': 251, 'text': 'optimised for analytical queries', 'document_ids': ['doc_0']}\n",
      "{'start': 264, 'end': 289, 'text': 'transactional operations.', 'document_ids': ['doc_0']}\n",
      "{'start': 316, 'end': 347, 'text': 'analyse historical data trends.', 'document_ids': ['doc_0']}\n",
      "\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Data warehousing is the process of collecting, sto...', 'title': 'de-concepts', 'url': '/home/notebook-user/response-eval-ai/sample-docs/understanding-de-concepts.md'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cohere.StreamingChat {\n",
       "\tresponse: <Response [200]>\n",
       "\ttexts: ['Data warehousing involves the process of collecting, storing and managing data from various sources in order to aid decision-making processes. \\n\\nData warehouses are different from traditional databases because they are optimised for analytical queries rather than transactional operations. They enable businesses to analyse historical data trends.']\n",
       "\tresponse_id: b6b0ca05-72eb-4e83-acca-c8a4f62d09d3\n",
       "\tconversation_id: e00efd1c-f468-4ed5-8b57-79b71f78d86e\n",
       "\tgeneration_id: 0026e4e4-3abf-416c-b785-4bd008ea3ea7\n",
       "\tpreamble: None\n",
       "\tprompt: None\n",
       "\tchat_history: [{'role': 'USER', 'message': 'What is data warehousing and how is it different from traditional database?'}, {'role': 'CHATBOT', 'message': 'Data warehousing involves the process of collecting, storing and managing data from various sources in order to aid decision-making processes. \\n\\nData warehouses are different from traditional databases because they are optimised for analytical queries rather than transactional operations. They enable businesses to analyse historical data trends.'}]\n",
       "\tfinish_reason: COMPLETE\n",
       "\ttoken_count: {'prompt_tokens': 1004, 'response_tokens': 54, 'total_tokens': 1058, 'billed_tokens': 367}\n",
       "\tmeta: {'api_version': {'version': '1'}, 'billed_units': {'input_tokens': 313, 'output_tokens': 54}}\n",
       "\tis_search_required: None\n",
       "\tcitations: [{'start': 41, 'end': 51, 'text': 'collecting', 'document_ids': ['doc_0']}, {'start': 53, 'end': 60, 'text': 'storing', 'document_ids': ['doc_0']}, {'start': 65, 'end': 99, 'text': 'managing data from various sources', 'document_ids': ['doc_0']}, {'start': 112, 'end': 142, 'text': 'aid decision-making processes.', 'document_ids': ['doc_0']}, {'start': 219, 'end': 251, 'text': 'optimised for analytical queries', 'document_ids': ['doc_0']}, {'start': 264, 'end': 289, 'text': 'transactional operations.', 'document_ids': ['doc_0']}, {'start': 316, 'end': 347, 'text': 'analyse historical data trends.', 'document_ids': ['doc_0']}]\n",
       "\tdocuments: [{'filename': '/home/notebook-user/response-eval-ai/sample-docs/understanding-de-concepts.md', 'id': 'doc_0', 'text': 'Data warehousing is the process of collecting, storing, and managing data from various sources to support decision-making processes. Unlike traditional databases, data warehouses are optimized for analytical queries rather than transactional operations. They typically use a dimensional model with fact and dimension tables, and they often involve data transformation and cleansing processes. Data warehouses enable businesses to analyze historical data trends and make informed decisions based on', 'title': 'de-concepts'}]\n",
       "\tsearch_results: None\n",
       "\tsearch_queries: None\n",
       "\ttool_calls: None\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chatbot.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
