{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "import hnswlib\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.text import partition_text\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# co = cohere.Client(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Datastore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents.\n",
    "\n",
    "    Parameters:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    sources (list): A list of dictionaries representing the sources of the documents.\n",
    "    docs (list): A list of dictionaries representing the documents, with 'title', 'content', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the documents.\n",
    "    docs_len (int): The number of documents in the collection.\n",
    "    index (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the documents using the Cohere API.\n",
    "    index(): Indexes the documents for efficient retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents  # raw documents\n",
    "        self.chunks = []            # chunked version of documents\n",
    "        self.chunks_embs = []       # embeddings of chunked documents\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()  # load raw documents and break into chunks\n",
    "        self.embed() # generate embeddings for each chunk\n",
    "        self.index() # store embeddings in an index\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for source in self.raw_documents:\n",
    "            elements = partition_text(filename=source[\"filename\"])\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.chunks.append(\n",
    "                    {\n",
    "                        \"title\": source[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": source[\"filename\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.chunks_len = len(self.chunks)\n",
    "\n",
    "        for i in range(0, self.chunks_len, batch_size):\n",
    "            batch = self.chunks[i : min(i + batch_size, self.chunks_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            chunks_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.chunks_embs.extend(chunks_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing documents...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.chunks_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.chunks_embs, list(range(len(self.chunks_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
    "\n",
    "        return self.idx\n",
    "\n",
    "    def search_and_rerank(self, query: str) -> List[Dict[str, str]]:\n",
    "        # SEARCH\n",
    "        query_emb = co.embed(\n",
    "                  texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
    "              ).embeddings\n",
    "\n",
    "        chunk_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # RERANK\n",
    "        chunks_to_rerank = [self.chunks[chunk_id][\"text\"] for chunk_id in chunk_ids]\n",
    "\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=chunks_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v2.0\",\n",
    "        )\n",
    "\n",
    "        chunk_ids_reranked = [chunk_ids[result.index] for result in rerank_results]\n",
    "\n",
    "        chunks_retrieved = []\n",
    "        for chunk_id in chunk_ids_reranked:\n",
    "            chunks_retrieved.append(\n",
    "                {\n",
    "                \"title\": self.chunks[chunk_id][\"title\"],\n",
    "                \"text\": self.chunks[chunk_id][\"text\"],\n",
    "                \"filename\": self.chunks[chunk_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing documents...\n",
      "Indexing complete with 41 documents.\n"
     ]
    }
   ],
   "source": [
    "sources = [\n",
    "    {\n",
    "        \"title\": \"Data Quarkle\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md\"},\n",
    "    {\n",
    "        \"title\": \"RAGs\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/undestanding-rags.md\"},\n",
    "        {\n",
    "        \"title\": \"sample\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/sample.md\"},    \n",
    "        {\n",
    "        \"title\": \"de-concepts\", \n",
    "        \"filename\": \"/home/notebook-user/response-eval-ai/sample-docs/understanding-de-concepts.md\"}, \n",
    "]\n",
    "\n",
    "# Create an instance of the Datastore class with the given sources\n",
    "datastore = Datastore(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.NarrativeText at 0x7f45b04cbfd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f45b04cbd60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f45b04c4610>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f45b04c4070>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f45b04c67a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f45b04c6da0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_text(filename='/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Data Quarkle',\n",
       "  'text': '# Data Quarkle - a new concept Introducing \"DataQuarkle\" – a fantastical approach to data engineering that transcends conventional boundaries and unleashes the power of cosmic data manipulation.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'In the realm of DataQuarkle, innovation knows no bounds. From \"Datafusion\" spells that merge disparate data sources into a harmonious symphony of insights to \"Datamancy\" rituals that foresee future trends through the cosmic currents, DataQuarkle pushes the boundaries of what is possible in the realm of data engineering.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'},\n",
       " {'title': 'Data Quarkle',\n",
       "  'text': 'DataQuarkle harnesses the mysterious energy of \"Gigavortex,\" a celestial phenomenon that swirls through the cosmos, weaving together streams of data from across the universe. Powered by \"Quarkblasters,\" quantum-enhanced data processors, DataQuarkle pioneers the frontier of \"Datastral Travel,\" enabling data engineers to traverse the vast expanse of the Dataverse in the blink of an eye.',\n",
       "  'filename': '/home/notebook-user/response-eval-ai/sample-docs/concept-dataquarkle.md'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore.search_and_rerank(\"quarkle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, datastore: Datastore):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the Chatbot class.\n",
    "\n",
    "        Parameters:\n",
    "        storage (Storage): An instance of the Storage class.\n",
    "\n",
    "        \"\"\"\n",
    "        self.datastore = datastore\n",
    "        self.conversation_id = str(uuid.uuid4())\n",
    "\n",
    "    def run(self, ques_ans):\n",
    "        \"\"\"\n",
    "        Runs the chatbot application.\n",
    "\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Get the user message\n",
    "            message = ques_ans\n",
    "\n",
    "            # # Typing \"quit\" ends the conversation\n",
    "            # if message.lower() == \"quit\":\n",
    "            #     print(\"Ending chat.\")\n",
    "            #     break\n",
    "            # else:\n",
    "            #     print(f\"User: {message}\")\n",
    "\n",
    "            # Generate search queries, if any\n",
    "            response_queries = co.chat(message=message, search_queries_only=True)\n",
    "\n",
    "            if response_queries.search_queries:\n",
    "                print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "                # Get the query(s)\n",
    "                queries = []\n",
    "                for search_query in response_queries.search_queries:\n",
    "                    queries.append(search_query[\"text\"])\n",
    "\n",
    "                # Retrieve documents for each query\n",
    "                chunks = []\n",
    "                for query in queries:\n",
    "                    chunks.extend(self.datastore.search_and_rerank(query))\n",
    "            \n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    documents=chunks,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                response = co.chat(\n",
    "                    message=message,\n",
    "                    conversation_id=self.conversation_id,\n",
    "                    stream=True,\n",
    "                )\n",
    "\n",
    "            # Print the chatbot response\n",
    "            print(\"\\nChatbot:\")\n",
    "            \n",
    "            citations_flag = False\n",
    "            \n",
    "            for event in response:\n",
    "                                \n",
    "                # Text\n",
    "                if event.event_type == \"text-generation\":\n",
    "                    print(event.text, end=\"\")\n",
    "\n",
    "                # Citations\n",
    "                if event.event_type == \"citation-generation\":\n",
    "                    if not citations_flag:\n",
    "                        print(\"\\n\\nCITATIONS:\")\n",
    "                        citations_flag = True\n",
    "                    print(event.citations[0])\n",
    "            \n",
    "            # Documents\n",
    "            if citations_flag:\n",
    "                print(\"\\n\\nDOCUMENTS:\")\n",
    "                documents = [{'id': doc['id'],\n",
    "                                'text': doc['text'][:50] + '...',\n",
    "                                'title': doc['title'],\n",
    "                                'url': doc['filename']} \n",
    "                                for doc in response.documents]\n",
    "                for doc in documents:\n",
    "                    print(doc)\n",
    "\n",
    "            print(f\"\\n{'-'*100}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Given the following question and answer:\n",
      "\n",
      "Chatbot:\n",
      "Q: What is the capital of France? \n",
      "A: The capital of France is Paris.\n",
      "\n",
      "Can you rewrite the question in a different way, while keeping the same meaning?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "User: Given the question: What is data warehousing? Answer: Data warehouse is a place where data is stored for analytical queries and business decision making.  Is answer clear and coherent? Reply only in : yes, somewhat or no\n",
      "Retrieving information...\n",
      "Chatbot:\n",
      "Yes, the answer is clear and coherent.\n",
      "\n",
      "CITATIONS:\n",
      "{'start': 0, 'end': 3, 'text': 'Yes', 'document_ids': ['doc_0']}\n",
      "\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Data warehousing is the process of collecting, sto...', 'title': 'de-concepts', 'url': '/home/notebook-user/response-eval-ai/sample-docs/understanding-de-concepts.md'}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Ending chat.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Chatbot class with the Datastore instance\n",
    "chatbot = Chatbot(datastore)\n",
    "\n",
    "# # Run the chatbot\n",
    "# chatbot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
